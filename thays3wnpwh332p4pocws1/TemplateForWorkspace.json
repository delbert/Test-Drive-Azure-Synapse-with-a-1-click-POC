{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "thays3wnpwh332p4pocws1"
		},
		"TripFaresSynapseAnalyticsLinkedService_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'TripFaresSynapseAnalyticsLinkedService'"
		},
		"DatabaseLinkedService_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'DatabaseLinkedService'"
		},
		"StretchesLinkedService_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'StretchesLinkedService'"
		},
		"sinkLink_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'sinkLink'"
		},
		"HttpServerTripDataLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/tripDataAndFaresCSV/trip-data.csv"
		},
		"HttpServerTripFareDataLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/tripDataAndFaresCSV/fares-data.csv"
		},
		"TripFaresDataLakeStorageLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().datalakeAccountName,'.dfs.core.windows.net')}"
		},
		"keyVaultLinkedservice_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().keyVaultName,'.vault.azure.net/')}"
		},
		"DatabaseLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://thays3wnpwh332p4poc.dfs.core.windows.net"
		},
		"StretchesLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://thays3wnpwh332p4poc.dfs.core.windows.net"
		},
		"sinkLink_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://thays3wnpwh332p4poc.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresDataPipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "IngestTripDataIntoADLS",
						"description": "Copies the trip data csv file from the git repo and loads it into the ADLS.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.00:10:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "tripsDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "tripDataSink",
								"type": "DatasetReference",
								"parameters": {
									"datalakeAccountName": {
										"value": "@pipeline().parameters.datalakeAccountName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "IngestTripFaresDataIntoADLS",
						"description": "Copies the trip fare data csv file from the git repo and loads it into the ADLS.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.00:10:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "faresDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "faresDataSink",
								"type": "DatasetReference",
								"parameters": {
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"datalakeAccountName": {
										"value": "@pipeline().parameters.datalakeAccountName",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "JoinAndAggregateData",
						"description": "Reads the raw data from both CSV files inside the ADLS, performs the desired transformations (inner join and aggregation) and writes the transformed data into the synapse SQL pool.",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.00:30:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "tripFaresDataTransformations",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"TripDataCSV": {
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										},
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										}
									},
									"FaresDataCSV": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									},
									"SynapseAnalyticsSink": {
										"SchemaName": {
											"value": "@pipeline().parameters.SchemaName",
											"type": "Expression"
										},
										"SynapseWorkspaceName": {
											"value": "@pipeline().parameters.SynapseWorkspaceName",
											"type": "Expression"
										},
										"SQLDedicatedPoolName": {
											"value": "@pipeline().parameters.SQLDedicatedPoolName",
											"type": "Expression"
										},
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"SQLLoginUsername": {
											"value": "@pipeline().parameters.SQLLoginUsername",
											"type": "Expression"
										}
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Create Schema If Does Not Exists",
						"description": "Creates the schema inside the SQL dedicated pool. Shema name comes from the pipeline parameter 'SchemaName'.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "IngestTripDataIntoADLS",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "IngestTripFaresDataIntoADLS",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.00:05:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '@{pipeline().parameters.SchemaName}')\nBEGIN\nEXEC('CREATE SCHEMA @{pipeline().parameters.SchemaName}')\nselect Count(*) from sys.symmetric_keys;\nEND\nELSE\nBEGIN\n    select Count(*) from sys.symmetric_keys;\nEND",
									"type": "Expression"
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "azureSynapseAnalyticsSchema",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							},
							"firstRowOnly": false
						}
					}
				],
				"parameters": {
					"SchemaName": {
						"type": "string",
						"defaultValue": "tripFares"
					},
					"SynapseWorkspaceName": {
						"type": "string",
						"defaultValue": "thays3wnpwh332p4pocws1.database.windows.net"
					},
					"SQLDedicatedPoolName": {
						"type": "string",
						"defaultValue": "thays3wnpwh332p4pocws1p1"
					},
					"SQLLoginUsername": {
						"type": "string",
						"defaultValue": "dt"
					},
					"KeyVaultName": {
						"type": "string",
						"defaultValue": "kvthays3wnpwh332p4poc"
					},
					"datalakeAccountName": {
						"type": "string",
						"defaultValue": "thays3wnpwh332p4poc"
					}
				},
				"folder": {
					"name": "TripFaresDataPipeline"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/tripsDataSource')]",
				"[concat(variables('workspaceId'), '/datasets/tripDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSource')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSink')]",
				"[concat(variables('workspaceId'), '/dataflows/tripFaresDataTransformations')]",
				"[concat(variables('workspaceId'), '/datasets/azureSynapseAnalyticsSchema')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azureSynapseAnalyticsSchema')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azureSynapseAnalyticsTable')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SchemaName": {
						"type": "string"
					},
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().SchemaName",
						"type": "Expression"
					},
					"table": "AggregateTaxiData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/faresDataSink')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"datalakeAccountName": {
							"value": "@dataset().datalakeAccountName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"keyVaultName": {
						"type": "string",
						"defaultValue": "kvmsft"
					},
					"datalakeAccountName": {
						"type": "string",
						"defaultValue": "adlsmsft"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fares-data.csv",
						"fileSystem": "public"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "medallion",
						"type": "String"
					},
					{
						"name": "hack_license",
						"type": "String"
					},
					{
						"name": "vendor_id",
						"type": "String"
					},
					{
						"name": "pickup_datetime",
						"type": "String"
					},
					{
						"name": "payment_type",
						"type": "String"
					},
					{
						"name": "fare_amount",
						"type": "String"
					},
					{
						"name": "surcharge",
						"type": "String"
					},
					{
						"name": "mta_tax",
						"type": "String"
					},
					{
						"name": "tip_amount",
						"type": "String"
					},
					{
						"name": "tolls_amount",
						"type": "String"
					},
					{
						"name": "total_amount",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/faresDataSource')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServerTripFareDataLinkedService",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServerTripFareDataLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripDataSink')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"datalakeAccountName": {
							"value": "@dataset().datalakeAccountName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"datalakeAccountName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "trip-data.csv",
						"fileSystem": "public"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "medallion",
						"type": "String"
					},
					{
						"name": "hack_license",
						"type": "String"
					},
					{
						"name": "vendor_id",
						"type": "String"
					},
					{
						"name": "rate_code",
						"type": "String"
					},
					{
						"name": "store_and_fwd_flag",
						"type": "String"
					},
					{
						"name": "pickup_datetime",
						"type": "String"
					},
					{
						"name": "dropoff_datetime",
						"type": "String"
					},
					{
						"name": "passenger_count",
						"type": "String"
					},
					{
						"name": "trip_time_in_secs",
						"type": "String"
					},
					{
						"name": "trip_distance",
						"type": "String"
					},
					{
						"name": "pickup_longitude",
						"type": "String"
					},
					{
						"name": "pickup_latitude",
						"type": "String"
					},
					{
						"name": "dropoff_longitude",
						"type": "String"
					},
					{
						"name": "dropoff_latitude",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripsDataSource')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServerTripDataLinkedService",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServerTripDataLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServerTripDataLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServerTripDataLinkedService_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServerTripFareDataLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServerTripFareDataLinkedService_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresDataLakeStorageLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					},
					"datalakeAccountName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('TripFaresDataLakeStorageLinkedService_properties_typeProperties_url')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": {
									"value": "@linkedService().keyVaultName",
									"type": "Expression"
								}
							}
						},
						"secretName": "adlsAccessKey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresSynapseAnalyticsLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('TripFaresSynapseAnalyticsLinkedService_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": {
									"value": "@linkedService().keyVaultName",
									"type": "Expression"
								}
							}
						},
						"secretName": "synapseSqlLoginPassword"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/keyVaultLinkedservice')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('keyVaultLinkedservice_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripFaresDataTransformations')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TripFaresDataFlow"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "tripDataSink",
								"type": "DatasetReference"
							},
							"name": "TripDataCSV"
						},
						{
							"dataset": {
								"referenceName": "faresDataSink",
								"type": "DatasetReference"
							},
							"name": "FaresDataCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureSynapseAnalyticsTable",
								"type": "DatasetReference"
							},
							"name": "SynapseAnalyticsSink"
						}
					],
					"transformations": [
						{
							"name": "AggregateByPaymentType"
						},
						{
							"name": "InnerJoinWithTripFares"
						}
					],
					"script": "source(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\trate_code as string,\n\t\tstore_and_fwd_flag as string,\n\t\tpickup_datetime as string,\n\t\tdropoff_datetime as string,\n\t\tpassenger_count as string,\n\t\ttrip_time_in_secs as string,\n\t\ttrip_distance as string,\n\t\tpickup_longitude as string,\n\t\tpickup_latitude as string,\n\t\tdropoff_longitude as string,\n\t\tdropoff_latitude as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinferDriftedColumnTypes: true,\n\tignoreNoFilesFound: false) ~> TripDataCSV\nsource(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\tpickup_datetime as string,\n\t\tpayment_type as string,\n\t\tfare_amount as string,\n\t\tsurcharge as string,\n\t\tmta_tax as string,\n\t\ttip_amount as string,\n\t\ttolls_amount as string,\n\t\ttotal_amount as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinferDriftedColumnTypes: true,\n\tignoreNoFilesFound: false) ~> FaresDataCSV\nInnerJoinWithTripFares aggregate(groupBy(payment_type),\n\taverage_fare = avg(toInteger(total_amount)),\n\t\ttotal_trip_distance = sum(toInteger(trip_distance))) ~> AggregateByPaymentType\nTripDataCSV, FaresDataCSV join(TripDataCSV@medallion == FaresDataCSV@medallion\n\t&& TripDataCSV@hack_license == FaresDataCSV@hack_license\n\t&& TripDataCSV@vendor_id == FaresDataCSV@vendor_id\n\t&& TripDataCSV@pickup_datetime == FaresDataCSV@pickup_datetime,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~> InnerJoinWithTripFares\nAggregateByPaymentType sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\trecreate:true,\n\tformat: 'table',\n\tstaged: false,\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true,\n\terrorHandlingOption: 'stopOnFirstError') ~> SynapseAnalyticsSink"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/tripDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/azureSynapseAnalyticsTable')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AggregateTaxiData')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [payment_type]\n,[average_fare]\n,[total_trip_distance]\n FROM [tripFares].[AggregateTaxiData]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "thays3wnpwh332p4pocws1p1",
						"poolName": "thays3wnpwh332p4pocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/loadFare')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "DROP TABLE nyc.fare\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'fare' AND O.TYPE = 'U' AND S.NAME = 'nyc')\nCREATE TABLE nyc.fare\n\t(\n\t [medallion] nvarchar(4000),\n\t [hack_license] nvarchar(4000),\n\t [vendor_id] nvarchar(4000),\n\t [pickup_datetime] datetime2(0),\n\t [payment_type] nvarchar(4000),\n\t [fare_amount] float,\n\t [surcharge] float,\n\t [mta_tax] float,\n\t [tip_amount] float,\n\t [tolls_amount] float,\n\t [total_amount] float\n\t)\nWITH\n\t(\n\tDISTRIBUTION = ROUND_ROBIN,\n\t CLUSTERED COLUMNSTORE INDEX\n\t -- HEAP\n\t)\nGO\n\n--Uncomment the 4 lines below to create a stored procedure for data pipeline orchestration​\n--CREATE PROC bulk_load_fare\n--AS\n--BEGIN\nCOPY INTO nyc.fare\n(medallion 1, hack_license 2, vendor_id 3, pickup_datetime 4, payment_type 5, fare_amount 6, surcharge 7, mta_tax 8, tip_amount 9, tolls_amount 10, total_amount 11)\nFROM 'https://thays3wnpwh332p4poc.dfs.core.windows.net/public/fares-data.csv'\nWITH\n(\n\tFILE_TYPE = 'CSV'\n\t,MAXERRORS = 13\n\t,FIRSTROW = 2\n\t,ERRORFILE = 'https://thays3wnpwh332p4poc.dfs.core.windows.net/public/'\n)\n--END\nGO\n\nSELECT TOP 100 * FROM nyc.fare\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "thays3wnpwh332p4pocws1p1",
						"poolName": "thays3wnpwh332p4pocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/loadTrip')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'trip' AND O.TYPE = 'U' AND S.NAME = 'nyc')\nCREATE TABLE nyc.trip\n\t(\n\t [medallion] nvarchar(4000),\n\t [hack_license] nvarchar(4000),\n\t [vendor_id] nvarchar(4000),\n\t [rate_code] bigint,\n\t [store_and_fwd_flag] nvarchar(4000),\n\t [pickup_datetime] datetime2(0),\n\t [dropoff_datetime] datetime2(0),\n\t [passenger_count] bigint,\n\t [trip_time_in_secs] bigint,\n\t [trip_distance] float,\n\t [pickup_longitude] float,\n\t [pickup_latitude] float,\n\t [dropoff_longitude] float,\n\t [dropoff_latitude] float\n\t)\nWITH\n\t(\n\tDISTRIBUTION = ROUND_ROBIN,\n\t CLUSTERED COLUMNSTORE INDEX\n\t -- HEAP\n\t)\nGO\n\n--Uncomment the 4 lines below to create a stored procedure for data pipeline orchestration​\n--CREATE PROC bulk_load_trip\n--AS\n--BEGIN\nCOPY INTO nyc.trip\n(medallion 1, hack_license 2, vendor_id 3, rate_code 4, store_and_fwd_flag 5, pickup_datetime 6, dropoff_datetime 7, passenger_count 8, trip_time_in_secs 9, trip_distance 10, pickup_longitude 11, pickup_latitude 12, dropoff_longitude 13, dropoff_latitude 14)\nFROM 'https://thays3wnpwh332p4poc.dfs.core.windows.net/public/trip-data.csv'\nWITH\n(\n\tFILE_TYPE = 'CSV'\n\t,MAXERRORS = 13\n\t,FIRSTROW = 2\n\t,ERRORFILE = 'https://thays3wnpwh332p4poc.dfs.core.windows.net/public/'\n)\n--END\nGO\n\nSELECT TOP 100 * FROM nyc.trip\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "thays3wnpwh332p4pocws1p1",
						"poolName": "thays3wnpwh332p4pocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/queries')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--select count (*) from nyc.fare;\n\n--select count(*) from nyc.trip;\n\n--select * from sys.dm_pdw_exec_requests where status = 'Completed' option (label = 'monitor');\n\nselect * from sys.dm_pdw_exec_requests r where r.[label] = 'monitor';",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "thays3wnpwh332p4pocws1p1",
						"poolName": "thays3wnpwh332p4pocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/4499226a-31e0-4c35-a5f4-323521d98b5b/resourceGroups/thalamus/providers/Microsoft.Synapse/workspaces/thays3wnpwh332p4pocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://thays3wnpwh332p4pocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
							"\n",
							"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
							"The goal is to predict for a given trip whether there will be a tip or not.\n",
							"\n",
							" https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebook\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"from pyspark.sql.functions import unix_timestamp\n",
							"\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"from pyspark.ml import Pipeline\n",
							"from pyspark.ml import PipelineModel\n",
							"from pyspark.ml.feature import RFormula\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
							"from pyspark.ml.classification import LogisticRegression\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Ingest Data¶ \n",
							"\n",
							"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#To make development easier, faster and less expensive downsample for now\n",
							"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory Data Analysis\n",
							"\n",
							"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
							"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\n",
							"\n",
							"# Look at tips by amount count histogram\n",
							"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
							"ax1.set_title('Tip amount distribution')\n",
							"ax1.set_xlabel('Tip Amount ($)')\n",
							"ax1.set_ylabel('Counts')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# How many passengers tip'd by various amounts\n",
							"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
							"ax2.set_title('Tip amount by Passenger count')\n",
							"ax2.set_xlabel('Passenger count') \n",
							"ax2.set_ylabel('Tip Amount ($)')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# Look at the relationship between fare and tip amounts\n",
							"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
							"ax.set_title('Tip amount by Fare amount')\n",
							"ax.set_xlabel('Fare Amount ($)')\n",
							"ax.set_ylabel('Tip Amount ($)')\n",
							"plt.axis([-2, 80, -2, 20])\n",
							"plt.suptitle('')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization\n",
							"\n",
							"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
							"\n",
							"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
							"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
							"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
							"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
							"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
							"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
							"                                )\\\n",
							"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
							"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
							"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
							"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
							"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
							"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
							"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
							"                                )"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization Part 2\n",
							"\n",
							"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
							"\n",
							"Also create some more features based on new columns from the first round.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
							"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
							"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
							"                                                .otherwise(0).alias('trafficTimeBins')\n",
							"                                              )\\\n",
							"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Encoding\n",
							"\n",
							"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
							"\n",
							"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
							"\n",
							"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
							"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
							"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
							"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
							"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
							"\n",
							"# Create a new dataframe that has had the encodings applied\n",
							"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generation of Testing and Training Data Sets\n",
							"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Decide on the split between training and testing data from the dataframe \n",
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 1234\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train the Model\n",
							"\n",
							"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Create a new LR object for the model\n",
							"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
							"\n",
							"## The formula for the model\n",
							"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
							"\n",
							"## Undertake training and create an LR model\n",
							"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
							"\n",
							"## Saving the model is optional but its another for of inter session cache\n",
							"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
							"fileName = \"lrModel_\" + datestamp;\n",
							"logRegDirfilename = fileName;\n",
							"lrModel.save(logRegDirfilename)\n",
							"\n",
							"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
							"predictions = lrModel.transform(test_data_df)\n",
							"predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
							"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
							"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Evaluate and Visualize\n",
							"\n",
							"Plot the actual curve to develop a better understanding of the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
							"modelSummary = lrModel.stages[-1].summary\n",
							"\n",
							"plt.plot([0, 1], [0, 1], 'r--')\n",
							"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
							"         modelSummary.roc.select('TPR').collect())\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/thays3wnpwh332p4pocws1p1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"restorePointInTime": "0001-01-01T00:00:00",
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StretchDataFlow')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "tank-data"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "DatabaseLinkedService",
								"type": "LinkedServiceReference"
							},
							"name": "database"
						},
						{
							"linkedService": {
								"referenceName": "StretchesLinkedService",
								"type": "LinkedServiceReference"
							},
							"name": "stretchFiles"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "sinkLink",
								"type": "LinkedServiceReference"
							},
							"name": "sinkDb"
						},
						{
							"linkedService": {
								"referenceName": "sinkLink",
								"type": "LinkedServiceReference"
							},
							"name": "sinkStretch"
						}
					],
					"transformations": [
						{
							"name": "narrowColumnsDb"
						},
						{
							"name": "narrowColumnsStretch"
						}
					],
					"script": "source(output(\n\t\t{Material Cert} as string,\n\t\t{Cert #/SO#/Contract#} as string,\n\t\tCertReceived as date 'dd/MM/yyyy',\n\t\t{PO #} as integer '000',\n\t\tSULPHUR as float '##0.##',\n\t\t{Heat/Melt #-Code#} as string,\n\t\t{Test/IC #} as string,\n\t\t{Head ID/Plate ID} as string,\n\t\t{PN Code} as string,\n\t\tPN as integer '000',\n\t\t{Chart ID} as string,\n\t\t{Actual Thickness} as float '##0.##',\n\t\tMatlReceivedDate as date 'MM/dd/yyyy',\n\t\tInspector as string,\n\t\t{S/N} as string,\n\t\t{NB#} as integer '000',\n\t\tComments as string,\n\t\t{Hd or Plate} as string,\n\t\t{Material/Grade} as string,\n\t\tThk as float '##0.##',\n\t\t{Plate Producer} as string,\n\t\t{Plate Condition from Plate Vendor} as string,\n\t\t{Head Producer} as string,\n\t\t{Head Forming(H/C)} as string,\n\t\t{Heat Treatment} as string,\n\t\t{Comply to UCS-79(D), UG-81/SEC VIII,DIV.1} as boolean,\n\t\t{Hd Ht#=Plt Ht#} as boolean,\n\t\t{Yield Ok} as boolean,\n\t\t{Tensile Ok} as boolean,\n\t\tDateCertOKed as string,\n\t\t{Chemical  Ok} as boolean\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'delimited',\n\tfileSystem: 'tank-data',\n\tfolderPath: 'database',\n\tfileName: 'DatabaseData.csv',\n\tcolumnDelimiter: ',',\n\tescapeChar: '\\\\',\n\tquoteChar: '\\\"',\n\tcolumnNamesAsHeader: true) ~> database\nsource(output(\n\t\trec_num as integer,\n\t\t{ } as string,\n\t\tCIT_TIME as string,\n\t\tD_MIN as short,\n\t\tPSIG_1 as float,\n\t\tPSIG_2 as string,\n\t\tS1b as float,\n\t\tS1_A as float,\n\t\tS1_B as float,\n\t\tS1_C as float '##0.##',\n\t\tS1_D as string,\n\t\tS2b as float,\n\t\tS2_A as float,\n\t\tS2_B as float,\n\t\tS2_C as float '##0.##',\n\t\tS2_D as string,\n\t\tS3b as float,\n\t\tS3_A as float,\n\t\tS3_B as float,\n\t\tS3_C as float '##0.##',\n\t\tS3_D as string,\n\t\tS4b as float,\n\t\tS4_A as float,\n\t\tS4_B as float,\n\t\tS4_C as float '##0.##',\n\t\tS4_D as string,\n\t\tS5b as float,\n\t\tS5_A as float,\n\t\tS5_B as float,\n\t\tS5_C as float '##0.##',\n\t\tS5_D as string,\n\t\tS6b as string,\n\t\tS6_A as string,\n\t\tS6_B as string,\n\t\tS6_C as float '##0.##',\n\t\tS6_D as string,\n\t\tS7b as string,\n\t\tS7_A as string,\n\t\tS7_B as string,\n\t\tS7_C as float '##0.##',\n\t\tS7_D as string,\n\t\tS8b as string,\n\t\tS8_A as string,\n\t\tS8_B as string,\n\t\tS8_C as float '##0.##',\n\t\tS8_D as string,\n\t\tS9b as string,\n\t\tS9_A as string,\n\t\tS9_B as string,\n\t\tS9_C as float '##0.##',\n\t\tS9_D as string,\n\t\tS10b as string,\n\t\tS10_A as string,\n\t\tS10_B as string,\n\t\tS10_C as float,\n\t\tS10_D as string,\n\t\tS11b as string,\n\t\tS11_A as string,\n\t\tS11_B as string,\n\t\tS11_C as float '##0.##',\n\t\tS11_D as string,\n\t\tS12b as string,\n\t\tS12_A as string,\n\t\tS12_B as string,\n\t\tS12_C as float '##0.##',\n\t\tS12_D as string,\n\t\tS13b as string,\n\t\tS13_A as string,\n\t\tS13_B as string,\n\t\tS13_C as float '##0.##',\n\t\tS13_D as string,\n\t\tS14b as string,\n\t\tS14_A as string,\n\t\tS14_B as string,\n\t\tS14_C as float '##0.##',\n\t\tS14_D as string,\n\t\tS15b as string,\n\t\tS15_A as string,\n\t\tS15_B as string,\n\t\tS15_C as float '##0.##',\n\t\tS15_D as string,\n\t\tS16b as string,\n\t\tS16_A as string,\n\t\tS16_B as string,\n\t\tS16_C as float '##0.##',\n\t\tS16_D as string,\n\t\tS17b as string,\n\t\tS17_A as string,\n\t\tS17_B as string,\n\t\tS17_C as float '##0.##',\n\t\tS17_D as string,\n\t\tS18b as string,\n\t\tS18_A as string,\n\t\tS18_B as string,\n\t\tS18_C as float '##0.##',\n\t\tS18_D as string,\n\t\tS19b as string,\n\t\tS19_A as string,\n\t\tS19_B as string,\n\t\tS19_C as float '##0.##',\n\t\tS19_D as string,\n\t\tS20b as string,\n\t\tS20_A as string,\n\t\tS20_B as string,\n\t\tS20_C as float '##0.##',\n\t\tS20_D as string,\n\t\tS21b as string,\n\t\tS21_A as string,\n\t\tS21_B as string,\n\t\tS21_C as float '##0.##',\n\t\tS21_D as string,\n\t\tS22b as string,\n\t\tS22_A as string,\n\t\tS22_B as string,\n\t\tS22_C as float '##0.##',\n\t\tS22_D as string,\n\t\tS23b as string,\n\t\tS23_A as string,\n\t\tS23_B as string,\n\t\tS23_C as float '##0.##',\n\t\tS23_D as string,\n\t\tS24b as string,\n\t\tS24_A as string,\n\t\tS24_B as string,\n\t\tS24_C as float '##0.##',\n\t\tS24_D as string,\n\t\tS25b as string,\n\t\tS25_A as string,\n\t\tS25_B as string,\n\t\tS25_C as float '##0.##',\n\t\tS25_D as string,\n\t\tS26b as string,\n\t\tS26_A as string,\n\t\tS26_B as string,\n\t\tS26_C as float '##0.##',\n\t\tS26_D as string,\n\t\tS27b as string,\n\t\tS27_A as string,\n\t\tS27_B as string,\n\t\tS27_C as float '##0.##',\n\t\tS27_D as string,\n\t\tS28b as string,\n\t\tS28_A as string,\n\t\tS28_B as string,\n\t\tS28_C as float '##0.##',\n\t\tS28_D as string,\n\t\tS29b as string,\n\t\tS29_A as string,\n\t\tS29_B as string,\n\t\tS29_C as float '##0.##',\n\t\tS29_D as string,\n\t\tS30b as string,\n\t\tS30_A as string,\n\t\tS30_B as string,\n\t\tS30_C as float '##0.##',\n\t\tS30_D as string,\n\t\tS31b as string,\n\t\tS31_A as string,\n\t\tS31_B as string,\n\t\tS31_C as float '##0.##',\n\t\tS31_D as string,\n\t\tS32b as string,\n\t\tS32_A as string,\n\t\tS32_B as string,\n\t\tS32_C as float '##0.##',\n\t\tS32_D as string,\n\t\tS33b as string,\n\t\tS33_A as string,\n\t\tS33_B as string,\n\t\tS33_C as float '##0.##',\n\t\tS33_D as string,\n\t\tS34b as string,\n\t\tS34_A as string,\n\t\tS34_B as string,\n\t\tS34_C as float '##0.##',\n\t\tS34_D as string,\n\t\tS35b as string,\n\t\tS35_A as string,\n\t\tS35_B as string,\n\t\tS35_C as float '##0.##',\n\t\tS35_D as string,\n\t\tS36b as string,\n\t\tS36_A as string,\n\t\tS36_B as string,\n\t\tS36_C as float '##0.##',\n\t\tS36_D as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\trowUrlColumn: 'fileName',\n\tformat: 'delimited',\n\tfileSystem: 'tank-data',\n\tfolderPath: 'stretch',\n\tcolumnDelimiter: ',',\n\tescapeChar: '\\\\',\n\tquoteChar: '\\\"',\n\tcolumnNamesAsHeader: true) ~> stretchFiles\ndatabase select(mapColumn(\n\t\tLink = {Material Cert},\n\t\t{Cert No} = {Cert #/SO#/Contract#},\n\t\t{Heat#} = {Heat/Melt #-Code#},\n\t\t{NB#},\n\t\t{Plate Producer}\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> narrowColumnsDb\nstretchFiles select(mapColumn(\n\t\tfileName,\n\t\trec_num,\n\t\teach(match(startsWith(name, \"S\") && endsWith(name, \"_C\")),\n\t\t\tsubstring($$, 2, length($$) - 3) = $$)\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> narrowColumnsStretch\nnarrowColumnsDb sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'delimited',\n\tfileSystem: 'tank-data',\n\tfolderPath: 'outputs',\n\tcolumnDelimiter: ',',\n\tescapeChar: '\\\\',\n\tquoteChar: '\\\"',\n\tcolumnNamesAsHeader: false,\n\tumask: 0022,\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> sinkDb\nnarrowColumnsStretch sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'delimited',\n\tfileSystem: 'tank-data',\n\tfolderPath: 'outputs',\n\tcolumnDelimiter: ',',\n\tescapeChar: '\\\\',\n\tquoteChar: '\\\"',\n\tcolumnNamesAsHeader: false,\n\tumask: 0022,\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> sinkStretch"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DatabaseLinkedService')]",
				"[concat(variables('workspaceId'), '/linkedServices/StretchesLinkedService')]",
				"[concat(variables('workspaceId'), '/linkedServices/sinkLink')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DatabaseLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('DatabaseLinkedService_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('DatabaseLinkedService_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StretchesLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('StretchesLinkedService_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('StretchesLinkedService_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sinkLink')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('sinkLink_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('sinkLink_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		}
	]
}